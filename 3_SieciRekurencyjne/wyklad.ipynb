{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): #sigmoid function\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def tanh(x): #tanh function tangens hiperblocziny\n",
    "    return np.tanh(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_s=5 #input size\n",
    "hidden_s=5 #hidden layer size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wagi dla komórek poniważ są 4 bramki\n",
    "Wf= np.random.random((hidden_s,hidden_s+input_s)) #wagi dla forget gate wielkość wyjścia ma być jak hidden size a drugi parametr to polaczone info\n",
    "Wis= np.random.random((hidden_s,hidden_s+input_s)) #wagi dla input s gate\n",
    "Wit= np.random.random((hidden_s,hidden_s+input_s)) #wagi dla input t gate\n",
    "Wo= np.random.random((hidden_s,hidden_s+input_s))#wagi dla output gate\n",
    "\n",
    "#biasy\n",
    "bf= np.random.random(hidden_s) #bias dla forget gate\n",
    "bis= np.random.random(hidden_s) #bias dla input s gate\n",
    "bit= np.random.random(hidden_s) #bias dla input t gate\n",
    "bo= np.random.random(hidden_s) #bias dla output gate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_h= np.zeros((1,hidden_s)) #stan ukryty\n",
    "stan_c= np.zeros((1,hidden_s)) #stan komórki\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja która wykona wszystko\n",
    "\n",
    "def LSTM(sh,sc,x, Wf, Wis, Wit, Wo, bf, bis, bit, bo):\n",
    "    #sh- stan ukryty\n",
    "    #sc- stan komórki\n",
    "    #x- wejście\n",
    "    #Wf- wagi forget gate\n",
    "    #Wis- wagi input s gate\n",
    "    #Wit- wagi input t gate\n",
    "    #Wo- wagi output gate\n",
    "    #bf- bias forget gate\n",
    "    #bis- bias input s gate\n",
    "    #bit- bias input t gate\n",
    "    #bo- bias output gate\n",
    "        \n",
    "    #połączenie sygnałów\n",
    "    sygnal = np.hstack((sh,x.reshape(1,-1))).T #połączenie stanu ukrytego i wejścia i transpozycja\n",
    "    #mnożenie\n",
    "    forgot= sigmoid(np.matmul(Wf,sygnal)+bf) #forget gate\n",
    "    \n",
    "    czy_dodac= sigmoid(np.matmul(Wis,sygnal)+bis) #czy dodać \n",
    "    ile_dodac= tanh(np.matmul(Wit,sygnal)+bit) #ile dodać do stanu komórki\n",
    "    \n",
    "    wyjscie=sigmoid(np.matmul(Wo,sygnal)+bo) #output gate \n",
    "    \n",
    "    #obliczeni stanu komórki stac\n",
    "    stan_c=sc*forgot + czy_dodac*ile_dodac\n",
    "    \n",
    "    #obliczenie stanu ukrytego\n",
    "    stan_h= wyjscie*tanh(stan_c)\n",
    "    return stan_c, stan_h #LSTM zwraca wyjściową wartość neuronu również jako pierwszy argument więc na przykład można podłączyć dense pod to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56371095, 0.98800784, 0.24818815, 0.34363388, 0.20235982])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "#potrzebujemy jakieś dane jedne ciąg danych\n",
    "X = np.random.random(input_s)\n",
    "X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uruchomienie\n",
    "stan_h, stan_c=  LSTM(stan_h,stan_c,X,Wf,Wis,Wit,Wo,bf,bis,bit,bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8031219 , 0.80037781, 0.87906013, 0.87783282, 0.88353492],\n",
       "       [0.67437165, 0.6745469 , 0.78942643, 0.77588426, 0.80109869],\n",
       "       [0.78790409, 0.78675153, 0.86951836, 0.86376851, 0.87601074],\n",
       "       [0.73793805, 0.73648349, 0.835181  , 0.82826109, 0.84310025],\n",
       "       [0.70667396, 0.7000365 , 0.81172229, 0.81711895, 0.81557777]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stan_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6298352 , 0.5957306 , 0.62315833, 0.64079839, 0.65579689],\n",
       "       [0.50991418, 0.44950334, 0.48538073, 0.51219248, 0.54751129],\n",
       "       [0.58919913, 0.53263633, 0.55260885, 0.58002577, 0.60641442],\n",
       "       [0.58040216, 0.53824608, 0.5739133 , 0.59379159, 0.61679266],\n",
       "       [0.53749851, 0.47719405, 0.51268393, 0.54605186, 0.56757478]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stan_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#możemy oczywiście użyć kerasa aby ułątwić sobie życie i nie pisać samodzielnie LSTM oraz metody uczenia wstecznej proagacji przez czas\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
