{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq2seq\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #potrzebne są jakieś dane za cel stawiamy sobie tłumaczenie\n",
    "with open('pol.txt', 'r', encoding='utf-8') as f:\n",
    "    file = f.read()\n",
    "    f.close()\n",
    "\n",
    "\n",
    "# input_tekst =[] #to będzie lista z tekstami wejściowymi\n",
    "# teksty_docelowe=[]#to będzie lista z tekstami docelowymi\n",
    "\n",
    "    \n",
    "# # linie=file.split('\\n') #to takie troche dokumenty tak to możemy rozmieć\n",
    "# # en,pl,_=linie[0][:10000].split('\\t') # w efelcie mam trzy listy z tekstami\n",
    "# # input_tekst.append(en)\n",
    "# # teksty_docelowe.append(\"\\t\"+pl+\"\\n\") #dodajemy znaczniki początku i końca tabulator to początek a nowa linia koniec\n",
    "# #tekst docelowy powinien mieć poczatek i koniec znaczniki w tekście taka by wiedzieć kiedy się kończy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testy i identyfiakcja błędu\n",
    "# for idw,wiersz in enumerate(linie):\n",
    "#     print(idw)\n",
    "#     en, pl, _ =wiersz[:min(10000,len(linie)-1)].split('\\t') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "teksty_docelowe=[]\n",
    "\n",
    "\n",
    "linie=file.split('\\n')\n",
    "linie=linie[:len(linie)-1] #ostatnia linia jest pusta\n",
    "\n",
    "\n",
    "#łownik zlozony nie ze słów tylko liter znaków\n",
    "chars=set()\n",
    "znaki=set()\n",
    "\n",
    "\n",
    "for wiersz in linie:\n",
    "    en,pl,_=wiersz[:min(10000,len(linie)-1)].split('\\t') \n",
    "    input_texts.append(en)\n",
    "    teksty_docelowe.append(\"\\t\"+pl+\"\\n\")\n",
    "#chcemy działać na literach nie na całych słowach\n",
    "    for char in en:\n",
    "        chars.add(char)\n",
    "    for znak in pl:\n",
    "        znaki.add(znak)\n",
    "        \n",
    "znaki.add('\\t')\n",
    "znaki.add('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(262, 315)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zwróćmy na uwage na wlikości słownika\n",
    "\n",
    "no_chars = len(chars)\n",
    "no_chars\n",
    "ile_znkaow=len(znaki)\n",
    "ile_znkaow\n",
    "#powinniśmy to teraz w pewny sposób zakodować więc musimy nzkeźć najdłuższy znak\n",
    "\n",
    "longest_seq= max([len(seq) for seq in input_texts])\n",
    "najdluzsza_sek=max([len(seq) for seq in teksty_docelowe])\n",
    "\n",
    "longest_seq,najdluzsza_sek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przyisanie znakom jakieś indeksy\n",
    "char2ind = {}\n",
    "for id ,c in enumerate(chars):\n",
    "    char2ind[c]=id\n",
    "    \n",
    "    \n",
    "znak2ind = {}\n",
    "for id ,c in enumerate(znaki):\n",
    "    znak2ind[c]=id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liczba danych, długosć sekwecnji,liczba słow w złowiku mamy takie wymiary\n",
    "\n",
    "X_en= np.zeros((20000,longest_seq,no_chars))\n",
    "X_pl= np.zeros((20000,najdluzsza_sek,ile_znkaow))\n",
    "Y= np.zeros((20000,najdluzsza_sek,ile_znkaow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'\\t'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m X_en[idd,idc\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:,char2ind[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m  \u001b[38;5;66;03m#cokolwiek tam jest to uzupełniamy zeramspacjami taki paddingi\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idz,z \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(d_pl):\n\u001b[1;32m---> 10\u001b[0m     X_pl[idd,idz,\u001b[43mchar2ind\u001b[49m\u001b[43m[\u001b[49m\u001b[43mz\u001b[49m\u001b[43m]\u001b[49m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;66;03m#jaki to ejst dkument idd, jaki znak idc, jaki znak char2ind[c] jaki token\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idz\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     12\u001b[0m         Y[idd,idz\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,char2ind[z]]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;66;03m#jaki to ejst dkument idd, jaki znak idc, jaki znak char2ind[c] jaki token przsuniecie o 1 w tył\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: '\\t'"
     ]
    }
   ],
   "source": [
    "#uzupełniamy te tablice wartościami\n",
    "\n",
    "for idd,(d_en,d_pl) in enumerate(zip(input_texts[:20000],teksty_docelowe[:20000])): #numer dokumentu, texty wiersze w językach jest tutaj niedokładność bo zip wykonuje pętle do krótszego ale nieważne to teraz\n",
    "    #petla po znkaach\n",
    "    for idc,c in enumerate(d_en):\n",
    "        X_en[idd,idc,char2ind[c]]=1.0 #jaki to ejst dkument idd, jaki znak idc, jaki znak char2ind[c] jaki token\n",
    "    X_en[idd,idc+1:,char2ind[\" \"]]=1.0  #cokolwiek tam jest to uzupełniamy zeramspacjami taki paddingi\n",
    "    \n",
    "    for idz,z in enumerate(d_pl):\n",
    "        X_pl[idd,idz,char2ind[z]]=1.0 #jaki to ejst dkument idd, jaki znak idc, jaki znak char2ind[c] jaki token\n",
    "        if idz>0:\n",
    "            Y[idd,idz-1,char2ind[z]]=1.0 #jaki to ejst dkument idd, jaki znak idc, jaki znak char2ind[c] jaki token przsuniecie o 1 w tył\n",
    "    X_pl[idd,idz+1:,char2ind[\" \"]]=1.0 \n",
    "    Y[idd,idz :,char2ind[\" \"]]=1.0 #dodajemy paddingi\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copilot fix\n",
    "input_texts = []\n",
    "teksty_docelowe = []\n",
    "\n",
    "linie = file.split('\\n')\n",
    "linie = linie[:len(linie) - 1]  # ostatnia linia jest pusta\n",
    "\n",
    "# słownik złożony nie ze słów tylko liter znaków\n",
    "znaki = set()\n",
    "\n",
    "for wiersz in linie:\n",
    "    en, pl, _ = wiersz[:min(10000, len(linie) - 1)].split('\\t')\n",
    "    input_texts.append(en)\n",
    "    teksty_docelowe.append(\"\\t\" + pl + \"\\n\")\n",
    "    # chcemy działać na literach nie na całych słowach\n",
    "    for char in en:\n",
    "        znaki.add(char)\n",
    "    for znak in pl:\n",
    "        znaki.add(znak)\n",
    "\n",
    "znaki.add('\\t')\n",
    "znaki.add('\\n')\n",
    "znaki.add(' ')  # Dodaj spację do zbioru znaków\n",
    "\n",
    "# Tworzenie słownika char2ind\n",
    "char2ind = {char: idx for idx, char in enumerate(sorted(znaki))}\n",
    "\n",
    "# Inicjalizacja tablic (przykładowe wymiary, dostosuj do swoich danych)\n",
    "num_samples = 20000\n",
    "max_len_en = max(len(text) for text in input_texts[:num_samples])\n",
    "max_len_pl = max(len(text) for text in teksty_docelowe[:num_samples])\n",
    "num_chars = len(char2ind)\n",
    "\n",
    "X_en = np.zeros((num_samples, max_len_en, num_chars), dtype='float32')\n",
    "X_pl = np.zeros((num_samples, max_len_pl, num_chars), dtype='float32')\n",
    "Y = np.zeros((num_samples, max_len_pl, num_chars), dtype='float32')\n",
    "\n",
    "# Uzupełnianie tablic wartościami\n",
    "for idd, (d_en, d_pl) in enumerate(zip(input_texts[:num_samples], teksty_docelowe[:num_samples])):\n",
    "    for idc, c in enumerate(d_en):\n",
    "        if c in char2ind:\n",
    "            X_en[idd, idc, char2ind[c]] = 1.0\n",
    "    X_en[idd, idc+1:, char2ind[\" \"]] = 1.0  # Padding\n",
    "\n",
    "    for idz, z in enumerate(d_pl):\n",
    "        if z in char2ind:\n",
    "            X_pl[idd, idz, char2ind[z]] = 1.0\n",
    "        if idz > 0 and z in char2ind:\n",
    "            Y[idd, idz-1, char2ind[z]] = 1.0\n",
    "    X_pl[idd, idz+1:, char2ind[\" \"]] = 1.0  # Padding\n",
    "    Y[idd, idz:, char2ind[\" \"]] = 1.0  # Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#na enkoder test w jezyku naglieskim\n",
    "enc_i=Input(shape=(None,no_chars))\n",
    "enc_h= LSTM(256,return_state=True)# ile chcemy wymiarów iterazcji, zeby zwracał też stany ukryte\n",
    "enc_out,state_h,state_c = enc_h(enc_i) #state_h to stan ukryty, state_c to stan komórki\n",
    "\n",
    "\n",
    "\n",
    "#dekoder jezyk polski\n",
    "dec_i=Input(shape=(None,ile_znkaow))\n",
    "dec_h= LSTM(256,return_sequences=True,return_state=True)\n",
    "dec_out, _,_=dec_h(dec_i,initial_state=[state_h,state_c]) #dodajemy stan ukryty\n",
    "#warstwa wyjściowa\n",
    "dec_dense=Dense(ile_znkaow,activation='softmax')(dec_out)\n",
    "\n",
    "\n",
    "model=Model([enc_i,dec_i],dec_out)\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_1\" is incompatible with the layer: expected shape=(None, None, 79), found shape=(32, 24, 105)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hostory\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_en\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX_pl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"functional_1\" is incompatible with the layer: expected shape=(None, None, 79), found shape=(32, 24, 105)"
     ]
    }
   ],
   "source": [
    "hostory=model.fit([X_en,X_pl],Y,epochs=10,validation_split=0.2, ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copilot fix\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "input_texts = []\n",
    "teksty_docelowe = []\n",
    "\n",
    "linie = file.split('\\n')\n",
    "linie = linie[:len(linie) - 1]  # ostatnia linia jest pusta\n",
    "\n",
    "# słownik złożony nie ze słów tylko liter znaków\n",
    "znaki = set()\n",
    "\n",
    "for wiersz in linie:\n",
    "    en, pl, _ = wiersz[:min(10000, len(linie) - 1)].split('\\t')\n",
    "    input_texts.append(en)\n",
    "    teksty_docelowe.append(\"\\t\" + pl + \"\\n\")\n",
    "    # chcemy działać na literach nie na całych słowach\n",
    "    for char in en:\n",
    "        znaki.add(char)\n",
    "    for znak in pl:\n",
    "        znaki.add(znak)\n",
    "\n",
    "znaki.add('\\t')\n",
    "znaki.add('\\n')\n",
    "znaki.add(' ')  # Dodaj spację do zbioru znaków\n",
    "\n",
    "# Tworzenie słownika char2ind\n",
    "char2ind = {char: idx for idx, char in enumerate(sorted(znaki))}\n",
    "\n",
    "# Konwersja tekstów na sekwencje indeksów\n",
    "input_sequences = [[char2ind[char] for char in text] for text in input_texts]\n",
    "target_sequences = [[char2ind[char] for char in text] for text in teksty_docelowe]\n",
    "\n",
    "# Padding sekwencji\n",
    "max_len_en = max(len(seq) for seq in input_sequences)\n",
    "max_len_pl = max(len(seq) for seq in target_sequences)\n",
    "\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_len_en, padding='post', value=char2ind[' '])\n",
    "target_sequences = pad_sequences(target_sequences, maxlen=max_len_pl, padding='post', value=char2ind[' '])\n",
    "\n",
    "# Inicjalizacja tablic (przykładowe wymiary, dostosuj do swoich danych)\n",
    "num_samples = len(input_sequences)\n",
    "num_chars = len(char2ind)\n",
    "\n",
    "X_en = np.zeros((num_samples, max_len_en, num_chars), dtype='float32')\n",
    "X_pl = np.zeros((num_samples, max_len_pl, num_chars), dtype='float32')\n",
    "Y = np.zeros((num_samples, max_len_pl, num_chars), dtype='float32')\n",
    "\n",
    "# Uzupełnianie tablic wartościami\n",
    "for idd, (d_en, d_pl) in enumerate(zip(input_sequences, target_sequences)):\n",
    "    for idc, c in enumerate(d_en):\n",
    "        X_en[idd, idc, c] = 1.0\n",
    "\n",
    "    for idz, z in enumerate(d_pl):\n",
    "        X_pl[idd, idz, z] = 1.0\n",
    "        if idz > 0:\n",
    "            Y[idd, idz-1, z] = 1.0\n",
    "\n",
    "# Sprawdzenie kształtów danych\n",
    "print(f'X_en shape: {X_en.shape}')\n",
    "print(f'X_pl shape: {X_pl.shape}')\n",
    "print(f'Y shape: {Y.shape}')\n",
    "\n",
    "# Dopasowanie modelu\n",
    "history = model.fit([X_en, X_pl], Y, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
